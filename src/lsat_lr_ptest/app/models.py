from datetime import datetime
from pydantic import BaseModel, Field

# --- Request Models ---

class GenerateRequest(BaseModel):
    """Request to generate a new test."""
    prompt: str = Field(..., description="The user-provided prompt to generate a test from.")

class AnswerItem(BaseModel):
    """Represents a single answer provided by a user for a question."""
    question_number: int = Field(..., description="The unique identifier for the question.")
    selected_answer: str = Field(..., description="The answer selected by the user (e.g., 'A', 'B').")

class SaveTestRequest(BaseModel):
    """Request to save a user's answers for a test."""
    test_id: int = Field(..., description="The ID of the test being saved.")
    answers: list[AnswerItem] = Field(..., description="A list of answers provided by the user.")


# --- Data Structures / Components (used in responses) ---

class SampleQuestion(BaseModel):
    """
    Represents a sample question generated by the LLM.
    This structure is also used by the GeminiClient.
    """
    stimulus: str = Field(..., description="The stimulus text for the question.")
    prompt: str = Field(..., description="The question prompt itself.")
    explanation: str = Field(..., description="An explanation for the question/answer.")

    class Config:
        validate_by_name = True # Retained from your version
        from_attributes = True  # Useful if ever creating from ORM objects or other attribute-based objects

class Question(BaseModel): # Renamed from SimilarQuestion for broader use
    """Represents a question with its details."""
    question_number: int = Field(..., description="Unique ID of the question.")
    stimulus: str | None = Field(None, description="Stimulus text, if any.") # Allow for optional stimulus
    prompt: str = Field(..., description="Question prompt.")
    answers: list[str] = Field(..., description="List of possible answer choices (e.g., ['A', 'B', 'C', 'D', 'E']).")
    correct_answer: str = Field(..., description="The correct answer choice (e.g., 'A').")
    explanation: str | None = Field(None, description="Explanation for the question, if any.") # Allow for optional explanation

    class Config:
        from_attributes = True

class UserAnsweredQuestion(Question): # Inherits from Question, adds user's answer
    """Represents a question as part of a test, including the user's selected answer."""
    selected_answer: str | None = Field(None, description="The user's selected answer for this question, if any.")

    class Config:
        from_attributes = True


# --- Response Models for Endpoints ---

class GenerateTestResponse(BaseModel): # Renamed from GenerateResponse
    """Response after generating a new test."""
    test_id: int = Field(..., description="ID of the newly generated test.")
    sample_question: SampleQuestion = Field(..., description="The LLM-generated sample question.")
    # Uses Question as the base for similar questions
    similar_questions: list[Question] = Field(..., description="List of similar questions found from the database.")
    distances: list[float] = Field(..., description="Corresponding distances from the vector search for similar questions.")

class TestSummary(BaseModel):
    """Summary information for a single test attempt."""
    test_id: int = Field(..., description="The unique identifier for the test.")
    prompt: str = Field(..., description="The prompt used to generate the test.")
    # Using datetime here allows Pydantic to handle validation and serialization to ISO string
    created_at: datetime = Field(..., description="Timestamp of when the test was created.")
    score: str = Field(..., description="The score achieved on the test, formatted as 'correct/total'.")

    class Config:
        from_attributes = True

class TestDetails(BaseModel): # Renamed from TestDetailResponse
    """Detailed information about a specific test, including all its questions and user answers."""
    test_id: int = Field(..., description="The unique identifier for the test.")
    prompt: str = Field(..., description="The prompt used to generate the test.")
    created_at: datetime = Field(..., description="Timestamp of when the test was created.")
    questions: list[UserAnsweredQuestion] = Field(..., description="List of questions in the test with user answers.")
    score: str = Field(..., description="The overall score for the test, formatted as 'correct/total'.")

    class Config:
        from_attributes = True

class QuestionListItem(BaseModel):
    """Represents a question in a paginated list (minimal details)."""
    question_number: int = Field(..., description="Unique identifier for the question.")
    prompt: str = Field(..., description="The main prompt of the question.")

    class Config:
        from_attributes = True

class QuestionList(BaseModel): # Renamed from QuestionListResponse
    """Response for a paginated list of questions."""
    questions: list[QuestionListItem] = Field(..., description="A list of question items.")
    # Optional: Add total count for pagination metadata
    # total: int = Field(..., description="Total number of questions available.")
    # limit: int = Field(..., description="The number of items per page.")
    # offset: int = Field(..., description="The offset of the current page.")

class StatusResponse(BaseModel):
    """Generic status response message."""
    status: str = Field(..., examples=["ok"], description="General status of the operation.")
    message: str | None = Field(None, examples=["Operation completed successfully."], description="An optional detailed message.")

class HealthStatus(BaseModel):
    """Response for the health check endpoint."""
    status: str = Field(..., examples=["ok"], description="Overall service status.")
    database_status: str = Field(..., examples=["connected"], description="Status of the database connection.")